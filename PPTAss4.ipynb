{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f8e90e-364f-40d7-a05c-ace3642fccc9",
   "metadata": {},
   "source": [
    "General Linear Model:\n",
    "\n",
    "1. What is the purpose of the General Linear Model (GLM)?\n",
    "\n",
    "SOL:-The purpose of the General Linear Model (GLM) is to analyze and model the relationship between a dependent variable and one or more independent variables. The GLM is a flexible and powerful statistical framework that allows for the analysis of various types of data, including continuous, categorical, and count data.\n",
    "\n",
    "\n",
    "\n",
    "2. What are the key assumptions of the General Linear Model?\n",
    "\n",
    "SOL:-\n",
    "The key assumptions of the GLM are as follows:\n",
    "\n",
    "Linearity\n",
    "\n",
    "Independence\n",
    "\n",
    "Homoscedasticity\n",
    "\n",
    "Normality of residuals\n",
    "\n",
    "No multicollinearity\n",
    "\n",
    "\n",
    "\n",
    "3. How do you interpret the coefficients in a GLM?\n",
    "\n",
    "SOL:-\n",
    "Here are some general guidelines for interpreting coefficients in a GLM:\n",
    "\n",
    "Continuous Independent Variable\n",
    "\n",
    "Categorical Independent Variable (Dummy/Indicator Variable)\n",
    "\n",
    "Interaction Terms\n",
    "\n",
    "Log-transformed Independent Variables\n",
    "\n",
    "Standardized Coefficients\n",
    "\n",
    "\n",
    "\n",
    "4. What is the difference between a univariate and multivariate GLM?\n",
    "\n",
    "SOL:-\n",
    "\n",
    "Univariate GLM focuses on analyzing a single dependent variable, while multivariate GLM involves analyzing multiple dependent variables simultaneously.\n",
    "\n",
    "Univariate GLM estimates separate regression coefficients for each dependent variable, whereas multivariate GLM estimates a single set of regression coefficients that explain the relationships between the predictor variables and the set of dependent variables.\n",
    "\n",
    "\n",
    "\n",
    "5. Explain the concept of interaction effects in a GLM.\n",
    "\n",
    "SOL:-\n",
    "In a General Linear Model (GLM), interaction effects refer to the combined effect of two or more independent variables on the dependent variable that is different from their individual effects. In other words, an interaction effect occurs when the relationship between the dependent variable and one independent variable depends on the level or presence of another independent variable.\n",
    "\n",
    "\n",
    "\n",
    "6. How do you handle categorical predictors in a GLM?\n",
    "\n",
    "SOL:-\n",
    "Handling categorical predictors in a General Linear Model (GLM) requires appropriate encoding and interpretation. Here are the common approaches to handle categorical predictors in a GLM:\n",
    "\n",
    "Dummy Coding\n",
    "\n",
    "Effect Coding\n",
    "\n",
    "Polynomial Coding.\n",
    "\n",
    "\n",
    "\n",
    "7. What is the purpose of the design matrix in a GLM?\n",
    "\n",
    "SOL:-\n",
    "The design matrix, also known as the model matrix or the X matrix, is a fundamental component of a General Linear Model (GLM). Its purpose is to represent the relationships between the independent variables and the dependent variable in a structured and matrix form. The design matrix plays a crucial role in fitting the GLM and estimating the regression coefficients.\n",
    "\n",
    "\n",
    "8. How do you test the significance of predictors in a GLM?\n",
    "\n",
    "SOL:-Here are the general steps to test the significance of predictors in a GLM:\n",
    "\n",
    "Fit the GLM: Use an appropriate estimation method (e.g., least squares, maximum likelihood) to fit the GLM using your dataset. This involves specifying the model equation and estimating the regression coefficients.\n",
    "\n",
    "Obtain coefficient estimates and standard errors: The GLM estimation provides the estimates for the regression coefficients, along with their standard errors. The coefficient estimates represent the expected change in the dependent variable associated with a one-unit change in the corresponding predictor variable.\n",
    "\n",
    "Calculate test statistics: To test the significance of the predictors, you need to calculate a test statistic that compares the estimated coefficient to its standard error. The most common test statistic is the t-statistic, which is calculated as the ratio of the coefficient estimate to its standard error.\n",
    "\n",
    "Determine the degrees of freedom: The degrees of freedom for the test statistic depend on the sample size and the complexity of the model. In many cases, the degrees of freedom are equal to the number of observations minus the number of estimated parameters (coefficients) in the model.\n",
    "\n",
    "Determine the critical value or p-value: With the test statistic and the degrees of freedom, you can determine the critical value or p-value associated with the test statistic. The critical value can be obtained from a t-distribution table or calculated using statistical software. Alternatively, the p-value represents the probability of observing a test statistic as extreme as (or more extreme than) the calculated value, assuming the null hypothesis is true.\n",
    "\n",
    "\n",
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "\n",
    "SOL:-Type I Sums of Squares:\n",
    "\n",
    "Type I sums of squares are calculated by sequentially adding predictors to the model in a predetermined order.\n",
    "The order of predictor inclusion is typically based on theoretical or practical considerations.\n",
    "Type I sums of squares allocate the variation explained by each predictor while ignoring the effects of other predictors that have been included later in the model.\n",
    "The order of predictor inclusion can have a significant impact on the Type I sums of squares, making it important to carefully consider the chosen order.\n",
    "Type II Sums of Squares:\n",
    "\n",
    "Type II sums of squares consider the unique contribution of each predictor after controlling for the effects of other predictors in the model.\n",
    "Each predictor is tested for its contribution to the model, independently of the other predictors.\n",
    "Type II sums of squares take into account the order of predictor inclusion and adjust for the effects of other predictors.\n",
    "Type II sums of squares are appropriate when there are no interactions or when the model is balanced (equal sample sizes for all combinations of predictor levels).\n",
    "Type III Sums of Squares:\n",
    "\n",
    "Type III sums of squares are designed to handle situations with interactions or unbalanced data.\n",
    "Type III sums of squares examine the unique contribution of each predictor, considering the effects of all other predictors and their interactions in the model.\n",
    "Type III sums of squares account for the interrelationships among predictors and provide an unbiased estimate of each predictor's contribution.\n",
    "Type III sums of squares are less sensitive to the order of predictor inclusion compared to Type I sums of squares.\n",
    "Type III sums of squares are commonly used when there are interactions or when the model is unbalanced.\n",
    "\n",
    "\n",
    "\n",
    "10. Explain the concept of deviance in a GLM.\n",
    "\n",
    "SOL(10):-\n",
    "Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It is defined as the difference between the Saturated and Proposed Models and can be thought as how much variation in the data does our Proposed Model account for. Therefore, the lower the deviance, the better the model.\n",
    "\n",
    "\n",
    "Regression:\n",
    "\n",
    "11. What is regression analysis and what is its purpose?\n",
    "\n",
    "sol_{11}:-\n",
    "\n",
    "Regression analysis is the process where we train the model by using the data by the linear regression model or logistic integration model to find the relation between the dependent variable and the independent variable. It is mostly used to Predict the dependent variable and provide continuous or discrete value.\n",
    "\n",
    "\n",
    "\n",
    "12. What is the difference between simple linear regression and multiple linear regression?\n",
    "\n",
    "sol_{12}:-Simple linear regression is the regression when we predict the dependent variable by using the single independent variable End multiple linear regression is the regression where we predict the dependent variable by using the more than 1 independent variable.\n",
    "\n",
    "\n",
    "13. How do you interpret the R-squared value in regression?\n",
    "\n",
    "sol_{13}:-R square value is the ratio of sum of square of the values and sum of the squares End R ^2 value can be determined by only in the linear regression problem it is not used in classification problems and it provide the accuracy of the model.\n",
    "\n",
    "\n",
    "14. What is the difference between correlation and regression?\n",
    "\n",
    "sol_{14}:-Correlation is the defined as the relation between the independent variable and the dependent variable or two different independent variables and it shows how they are correlated to each other while regression is the  where we train the our model By using the data and predict the continuous value as an output.\n",
    "\n",
    "\n",
    "15. What is the difference between the coefficients and the intercept in regression?\n",
    "\n",
    "sol_{15}:-The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept; labeled as constant.\n",
    "\n",
    "\n",
    "16. How do you handle outliers in regression analysis?\n",
    "\n",
    "sol_{16}:-There are many possible approaches to dealing with outliers: removing them from the observations, treating them (for example, capping the extreme observations at a reasonable value), or using algorithms that are well-suited for dealing with such values on their own.\n",
    "\n",
    "\n",
    "\n",
    "17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "\n",
    "sol_{17}:-In summary, when there is a difference in variance between predictor variables, OLS tends to give higher variance for coefficients corresponding to predictors with higher variance, while Ridge Regression reduces the variance differences between coefficients by shrinking them towards zero.\n",
    "\n",
    "\n",
    "\n",
    "18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "\n",
    "sol_{18}:-\n",
    "Heteroskedastic refers to a condition in which the variance of the residual term, or error term, in a regression model varies widely. Homoskedastic refers to a condition in which the variance of the error term in a regression model is constant.\n",
    "\n",
    "\n",
    "19. How do you handle multicollinearity in regression analysis?\n",
    "\n",
    "sol_{19}:-By doing the feature Selection or by doing the Correlation between the independent variable and at last just use high correlated independent variable to determine the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "20. What is polynomial regression and when is it used?\n",
    "\n",
    "sol_{20}:-Polynomial regression is the regression where the data Is not in the linear structure or we can say that it is spread just like a Making a shape of any polynomial.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Loss function:\n",
    "\n",
    "21. What is a loss function and what is its purpose in machine learning?\n",
    "\n",
    "SOL(21):-Loss function is the function which describes the difference between the expected output and the actual output. Iq shows the how the our model is working on the data set.\n",
    "\n",
    "\n",
    "22. What is the difference between a convex and non-convex loss function?\n",
    "\n",
    "SOL(22):-A function is concave if -f is convex -- i.e. if the chord from x to y lies on or below the graph of f. It is easy to see that every linear function -- whose graph is a straight line -- is both convex and concave. A non-convex function \"curves up and down\" -- it is neither convex nor concave.\n",
    "\n",
    "\n",
    "\n",
    "23. What is mean squared error (MSE) and how is it calculated?\n",
    "\n",
    "SOL(23):-Headlines is calculated in the regression problem and it shows the error between the predicted output to the actual output. And it is calculated by the or we can say that it is the ratio of Some of the squires of difference between the Predict Point and actual point.\n",
    "\n",
    "\n",
    "\n",
    "24. What is mean absolute error (MAE) and how is it calculated?\n",
    "\n",
    "SOL(24):-Mean Absolute error is the difference between Actual output and the predicted output. It is used to find the in the regression model.\n",
    "\n",
    "\n",
    "\n",
    "25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "\n",
    "SOL(25):-Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value.\n",
    "\n",
    "\n",
    "\n",
    "26. How do you choose the appropriate loss function for a given problem?\n",
    "\n",
    "SOL(26):-The Mean Squared Error, or MSE, loss is the default loss to use for regression problems. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian.\n",
    "\n",
    "\n",
    "\n",
    "27. Explain the concept of regularization in the context of loss functions.\n",
    "\n",
    "SOL(27):-Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.\n",
    "\n",
    "\n",
    "\n",
    "28. What is Huber loss and how does it handle outliers?\n",
    "\n",
    "SOL(28):-In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss.\n",
    "\n",
    "\n",
    "\n",
    "29. What is quantile loss and when is it used?\n",
    "\n",
    "SOL(29):-A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times. For a set of predictions, the loss will be the average.\n",
    "\n",
    "\n",
    "\n",
    "30. What is the difference between squared loss and absolute loss?\n",
    "\n",
    "SOL(30):-For square loss, you will choose the estimated mean of y0, as the true mean minimizes square loss on average (where the average is taken across random samples of y0 subject to x=x0). For absolute loss, you will choose the estimated median.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Optimizer (GD):\n",
    "\n",
    "31. What is an optimizer and what is its purpose in machine learning?\n",
    "\n",
    "SOL(31):-An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. But it is a daunting task to choose the appropriate weights for the model.\n",
    "\n",
    "\n",
    "\n",
    "32. What is Gradient Descent (GD) and how does it work?\n",
    "\n",
    "SOL(32):-Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\n",
    "\n",
    "\n",
    "\n",
    "33. What are the different variations of Gradient Descent?\n",
    "\n",
    "SOL(33):-There are three types of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent and mini-batch gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "\n",
    "SOL(34):-Learning rate (also referred to as step size or the alpha) is the size of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High learning rates result in larger steps but risks overshooting the minimum.\n",
    "\n",
    "\n",
    "\n",
    "35. How does GD handle local optima in optimization problems?\n",
    "\n",
    "SOL(35):-\n",
    "\n",
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "\n",
    "SOL(36):-\n",
    "\n",
    "37. Explain the concept of batch size in GD and its impact on training.\n",
    "\n",
    "SOL(37):-The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset.\n",
    "\n",
    "\n",
    "38. What is the role of momentum in optimization algorithms?\n",
    "\n",
    "SOL(38):-Momentum aids in the optimization process's convergence by keeping the optimizer going in the same direction as previously, even if the gradient changes direction or becomes zero. This means that the optimizer can take greater steps toward the cost function's minimum, which can help it get there faster.\n",
    "\n",
    "\n",
    "\n",
    "39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "\n",
    "SOL(39):-Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets.\n",
    "\n",
    "\n",
    "\n",
    "40. How does the learning rate affect the convergence of GD?\n",
    "\n",
    "SOL(40):-With gradient descent, if the learning rate is too small, the weights will be updated very slowly hence convergence takes a lot of time even when the gradient is high. This is shown in the left side image below. If the learning rate is too high cost oscillates around the minima as shown in the right side image below.\n",
    "\n",
    "\n",
    "\n",
    "Regularization:\n",
    "\n",
    "41. What is regularization and why is it used in machine learning?\n",
    "\n",
    "SOL(41):-Regularization is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting by adding extra information to it. Sometimes the machine learning model performs well with the training data but does not perform well with the test data.\n",
    "\n",
    "\n",
    "\n",
    "42. What is the difference between L1 and L2 regularization?\n",
    "\n",
    "SOL(42):-\n",
    "L2 regularization takes the square of the weights, so the cost of outliers present in the data increases exponentially. L1 regularization takes the absolute values of the weights, so the cost only increases linearly. By this I mean the number of solutions to arrive at one point.\n",
    "\n",
    "\n",
    "43. Explain the concept of ridge regression and its role in regularization.\n",
    "\n",
    "SOL(43):-\n",
    "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.\n",
    "\n",
    "\n",
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "\n",
    "SOL(44):-In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\n",
    "\n",
    "\n",
    "\n",
    "45. How does regularization help prevent overfitting in machine learning models?\n",
    "\n",
    "SOL(45):-Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation.\n",
    "\n",
    "\n",
    "\n",
    "46. What is early stopping and how does it relate to regularization?\n",
    "\n",
    "SOL(46):-In Regularization by Early Stopping, we stop training the model when the performance of the model on the validation set is getting worse-increasing loss or decreasing accuracy or poorer values of the scoring metric.\n",
    "\n",
    "\n",
    "\n",
    "47. Explain the concept of dropout regularization in neural networks.\n",
    "\n",
    "SOL(47):-Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network.\n",
    "\n",
    "\n",
    "\n",
    "48. How do you choose the regularization parameter in a model?\n",
    "\n",
    "SOL(48):-\n",
    "we estimate several different Ridge regressions, with different values of the regularization parameter; on the validation set, we choose the best model (the regularization parameter which gives the lowest MSE on the validation set);\n",
    "\n",
    "\n",
    "49. What is the difference between feature selection and regularization?\n",
    "\n",
    "SOL(49):-\n",
    "Feature selection, also known as feature subset selection, variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference. Regularization, where we are constraining the solution space while doing optimization.\n",
    "\n",
    "\n",
    "50. What is the trade-off between bias and variance in regularized models?\n",
    "\n",
    "SOL(50):-\n",
    "“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff.\n",
    "\n",
    "\n",
    "\n",
    "SVM:\n",
    "\n",
    "51. What is Support Vector Machines (SVM) and how does it work?\n",
    "\n",
    "ANS(51):-In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.\n",
    "\n",
    "\n",
    "52. How does the kernel trick work in SVM?\n",
    "\n",
    "ANS(52):-Kernels Methods are employed in SVM (Support Vector Machines) which are used in classification and regression problems. The SVM uses what is called a “Kernel Trick” where the data is transformed and an optimal boundary is found for the possible outputs.\n",
    "\n",
    "\n",
    "53. What are support vectors in SVM and why are they important?\n",
    "\n",
    "ANS(53):-Support Vectors: These are the points that are closest to the hyperplane. A separating line will be defined with the help of these data points. Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin.\n",
    "\n",
    "\n",
    "\n",
    "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
    "\n",
    "\n",
    "ANS(54):-Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin.\n",
    "\n",
    "\n",
    "55. How do you handle unbalanced datasets in SVM?\n",
    "\n",
    "\n",
    "ANS(55):-We can use the make_classification() function to define a synthetic imbalanced two-class classification dataset. We will generate 10,000 examples with an approximate 1:100 minority to majority class ratio. Once generated, we can summarize the class distribution to confirm that the dataset was created as we expected.\n",
    "\n",
    "\n",
    "56. What is the difference between linear SVM and non-linear SVM?\n",
    "\n",
    "\n",
    "ANS(56):-\n",
    "Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points.\n",
    "\n",
    "\n",
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "\n",
    "ANS(57):-C parameter adds a penalty for each misclassified data point. If c is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications.\n",
    "\n",
    "\n",
    "58. Explain the concept of slack variables in SVM.\n",
    "\n",
    "ANS(58):-Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible.\n",
    "\n",
    "\n",
    "59. What is the difference between hard margin and soft margin in SVM?\n",
    "\n",
    "ANS(59):-The difference between a hard margin and a soft margin in SVMs lies in the separability of the data. If our data is linearly separable, we go for a hard margin. However, if this is not the case, it won't be feasible to do that.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "60. How do you interpret the coefficients in an SVM model?\n",
    "\n",
    "ANS(60):-Let's say the svm would find only one feature useful for separating the data, then the hyperplane would be orthogonal to that axis. So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation.\n",
    "\n",
    "\n",
    "\n",
    "Decision Trees:\n",
    "\n",
    "61. What is a decision tree and how does it work?\n",
    "\n",
    "ANS(61):-A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
    "\n",
    "\n",
    "62. How do you make splits in a decision tree?\n",
    "\n",
    "ANS(62):-For each split, calculate the entropy of each child node independently.\n",
    "Calculate the entropy of each split using the weighted average entropy of child nodes.\n",
    "Choose the split with the lowest entropy or the greatest gain in information.\n",
    "Repeat these steps to obtain homogeneous split nodes.\n",
    "\n",
    "\n",
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "\n",
    "ANS(63):-The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits. To put it into context, a decision tree is trying to create sequential questions such that it partitions the data into smaller groups.\n",
    "\n",
    "\n",
    "64. Explain the concept of information gain in decision trees.\n",
    "\n",
    "ANS(64):-Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node.\n",
    "\n",
    "\n",
    "65. How do you handle missing values in decision trees?\n",
    "\n",
    "ANS(65):-Surrogate splitting rules enable you to use the values of other input variables to perform a split for observations with missing values. Important Note : Tree Surrogate splitting rule method can impute missing values for both numeric and categorical variables.\n",
    "\n",
    "\n",
    "66. What is pruning in decision trees and why is it important?\n",
    "\n",
    "ANS(66):-Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood.\n",
    "\n",
    "\n",
    "67. What is the difference between a classification tree and a regression tree?\n",
    "\n",
    "ANS(67):-Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous.\n",
    "\n",
    "\n",
    "68. How do you interpret the decision boundaries in a decision tree?\n",
    "\n",
    "ANS(68):-The first node of the tree called the “root node” contains the number of instances of all the classes respectively. Basically, we have to draw a line called “decision boundary” that separates the instances of different classes into different regions called “decision regions”.\n",
    "\n",
    "\n",
    "69. What is the role of feature importance in decision trees?\n",
    "\n",
    "ANS(69):-A decision tree is explainable machine learning algorithm all by itself. Beyond its transparency, feature importance is a common way to explain built models as well. Coefficients of linear regression equation give a opinion about feature importance but that would fail for non-linear models.\n",
    "\n",
    "\n",
    "70. What are ensemble techniques and how are they related to decision trees?\n",
    "\n",
    "ANS(70):-\n",
    "Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner.\n",
    "\n",
    "\n",
    "Ensemble Techniques:\n",
    "\n",
    "71. What are ensemble techniques in machine learning?\n",
    "\n",
    "ANS(71):-Ensemble technique if the technique where we combine all the weaker model and make the stronger model to predict the value or dependent variable by the use of independent variable.Is simple techniques are of two types that is begging And boosting.\n",
    "\n",
    "72. What is bagging and how is it used in ensemble learning?\n",
    "\n",
    "ANS(72):-Begging is one of the one type of ensemble technique and in begging technique we train the Model by using the data set and this model is trained by the different muscle learning algorithms such as Linear regression decision tree support vector machine Nave buyers And it End it to predict the dependent variable values by taking the average of all these models and for classification problems it takes weightage of the output.\n",
    "\n",
    "\n",
    "73. Explain the concept of bootstrapping in bagging.\n",
    "\n",
    "ANS(73):-Bagging is composed of two parts: aggregation and bootstrapping. Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method. The learning algorithm is then run on the samples selected.\n",
    "\n",
    "74. What is boosting and how does it work?\n",
    "\n",
    "ANS(74):- \n",
    "Boosting is also the one of the technique of ensemble technique and in boosting we train the model by using the data set in a linear manner and these machine learning algorithms are linear regression decision tree SVM name buyers and in this posting technique we just print the model and improve the previous model by taking the advantage of the previous one.\n",
    "\n",
    "75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "\n",
    "ANS(75):-\n",
    "AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost.\n",
    "\n",
    "\n",
    "76. What is the purpose of random forests in ensemble learning?\n",
    "\n",
    "ANS(75):-\n",
    "Random forest algorithm is an ensemble learning technique combining numerous classifiers to enhance a model's performance. Random Forest is a supervised machine-learning algorithm made up of decision trees. Random Forest is used for both classification and regression problems.\n",
    "\n",
    "\n",
    "77. How do random forests handle feature importance?\n",
    "\n",
    "ANS(75):-The final feature importance, at the Random Forest level, is it's average over all the trees. The sum of the feature's importance value on each trees is calculated and divided by the total number of trees: RFfi sub(i)= the importance of feature i calculated from all trees in the Random Forest model.\n",
    "\n",
    "\n",
    "\n",
    "78. What is stacking in ensemble learning and how does it work?\n",
    "\n",
    "ANS(75):-\n",
    "Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance.\n",
    "\n",
    "\n",
    "79. What are the advantages and disadvantages of ensemble techniques?\n",
    "\n",
    "ANS(75):-\n",
    "Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data.\n",
    "\n",
    "\n",
    "80. How do you choose the optimal number of models in an ensemble?\n",
    "\n",
    "ANS(75):-Step 1 : Find the KS of individual models. ...\n",
    "Step 2: Index all the models for easy access. ...\n",
    "Step 3: Choose the first two models as the initial selection and set a correlation limit. ...\n",
    "Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b68e37-14a2-44e0-a86a-5a27e0b805a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
