{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd45e03c-78e1-4289-9a52-3c92248468a3",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?\n",
    "\n",
    "ANS:-\n",
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data. A single machine learning model might make prediction errors depending on the accuracy of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f071e-e6a1-4072-a205-7a7477f32abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c64c4c-faa7-407a-a432-34531049b689",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "ANS:-Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b8194-af2b-41b4-9e99-35676afaefad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ce49c5-df41-40f2-8fe0-9f3bb08b3984",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works.\n",
    "\n",
    "ANS;-Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03da77-c884-4c71-a61d-68ef06126fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b2e8f3a-e9a9-41e4-843f-b3eeabf5151a",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "ANS:-\n",
    "AdaBoost. AdaBoost works by fitting one weak learner after the other. \n",
    "\n",
    "Gradient tree boosting.\n",
    "\n",
    "eXtreme Gradient Boosting - XGBoost. \n",
    "\n",
    "LightGBM. \n",
    "\n",
    "CatBoost. \n",
    "\n",
    "Resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a10e36-63cf-405e-86ae-ef6ea3bd8442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "796c9646-57c2-4d72-a885-0b04cd6e54ea",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "ANS:-\n",
    "\n",
    "learning_rate\n",
    "\n",
    "n_estimators=100 (number of trees).\n",
    "\n",
    "max_depth=3.\n",
    "\n",
    "min_samples_split=2.\n",
    "\n",
    "min_samples_leaf=1.\n",
    "\n",
    "subsample=1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33c144-0635-4cb8-b049-d7e61b3bb655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d30e807-4520-4b0d-a52c-8ae02a671c11",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "ANS:-Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719c945-f047-447f-8562-8666a385562f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "891d2bba-b96f-409f-8946-8e4a4d76e014",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "ANS:-AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning. It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857b1c5-4b0e-4145-b63f-9d8a248ce72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e715ee8b-4966-42a4-96bd-919f4e62df85",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "ANS:-\n",
    "\n",
    "The error function that AdaBoost uses is an exponential loss function. First we find the products between the true values of training samples and the overall prediction for each sample. Then we take the sum of all the exponentials of these products in order to compute the error at iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca9cd5-0377-42dd-8b60-7b9238d736e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6b2847-e59d-49a6-87cb-3301b41615d8",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "ANS:-This is done by making misclassified cases to be updated with increased weights after an iteration. Increased weights would make our learning algorithm pay higher attention to these observations in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a36483-beae-4f09-b6a3-bf5e675de94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9854d614-f057-4d88-84ae-9b7a4c36c4e3",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "ANS:-The default number of estimators for AdaBoost in sklearn is 50. Using the exact same loop as above we can get the images of each of the 50 decision trees. There are the first three decision trees, each having a different split at the top node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829573e2-75d7-4a52-82c0-ecec3b9d80dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
