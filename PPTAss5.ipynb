{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2476b9c7-1824-44ac-bc4e-231c0ad71812",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Naive Approach:\n",
    "\n",
    "1. What is the Naive Approach in machine learning?\n",
    "\n",
    "ANS(1):-The Naive Bayes Algorithm is one of the crucial algorithms in machine learning that helps with classification problems. It is derived from Bayes' probability theory and is used for text classification, where you train high-dimensional datasets.\n",
    "\n",
    "\n",
    "2. Explain the assumptions of feature independence in the Naive Approach.\n",
    "\n",
    "ANS(2):-The assumption of feature independence in Naive Bayes simplifies the computation and makes the algorithm more efficient. By assuming independence, the joint probability of features given the class can be computed as the product of individual feature probabilities.\n",
    "\n",
    "\n",
    "\n",
    "3. How does the Naive Approach handle missing values in the data?\n",
    "\n",
    "ANS(3):-Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. What are the advantages and disadvantages of the Naive Approach?\n",
    "\n",
    "ANS(4):-\n",
    "\n",
    "The advantage is that it is inexpensive to develop, store data, and operate. The disadvantage is that it does not consider any possible causal relationships that underly the forecasted variable. This model adds the latest observed absolute period -to-period change to the most recent observed level of the variable.\n",
    "\n",
    "\n",
    "5. Can the Naive Approach be used for regression problems? If yes, how?\n",
    "\n",
    "ANS(5):-Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems, though with some modifications, it can also be used for solving regression problems.\n",
    "\n",
    "\n",
    "\n",
    "6. How do you handle categorical features in the Naive Approach?\n",
    "\n",
    "ANS(6):-Step 1: Drop columns with categorical data. You'll get started with the most straightforward approach. ...\n",
    "Step 2: Label encoding. Before jumping into label encoding, we'll investigate the dataset. ...\n",
    "Step 3: Investigating cardinality. ...\n",
    "Step 4: One-hot encoding.\n",
    "\n",
    "\n",
    "\n",
    "7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
    "\n",
    "ANS(7):-Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews.\n",
    "\n",
    "\n",
    "\n",
    "8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
    "\n",
    "ANS(1):-\n",
    "The default value for the minimum threshold of probabilities is 0.001. Valid values are positive numbers that are smaller than 1. The Naive Bayes method can only work with discrete input fields.\n",
    "\n",
    "\n",
    "\n",
    "9. Give an example scenario where the Naive Approach can be applied.\n",
    "\n",
    "ANS(1):-Some best examples of the Naive Bayes Algorithm are sentimental analysis, classifying new articles, and spam filtration. Classification algorithms are used for categorizing new observations into predefined classes for the uninitiated data. The Naive Bayes Algorithm is known for its simplicity and effectiveness.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "KNN:\n",
    "\n",
    "10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
    "\n",
    "ANS(10):-\n",
    "K nearest neighbor algorithm is one of the algorithm in machine learning which is used to solve the problem of classification as well as the regression problem and the method is calculate the dependent value by calculating the distance between the points and on this basis it gives the The predicted value.\n",
    "\n",
    "\n",
    "\n",
    "11. How does the KNN algorithm work?\n",
    "\n",
    "ANS(11):-GNN algorithm find the distance between the input given by the user to the other data points which is present and train the model so on this basis the input point is trace the distance between the nearest points which is present in the model and on this basis model provides the solution of classification as well as regression problem.\n",
    "\n",
    "\n",
    "\n",
    "12. How do you choose the value of K in KNN?\n",
    "\n",
    "ANS(12):-The value of K to find the expected value of dependent variable which is depend on independent variable or features and K value Can we find out why the cross validation method.\n",
    "\n",
    "\n",
    "\n",
    "13. What are the advantages and disadvantages of the KNN algorithm?\n",
    "\n",
    "ANS(13):-\n",
    "Advantages\t                                   Disadvantages\n",
    "Simple and Easy to Understand\t               Sensitive to Outliers\n",
    "Non-parametric\t                               Computationally Expensive\n",
    "No Training Required\t                       Requires Good Choice of K\n",
    "Can Handle Large Datasets\t                   Limited to Euclidean Distance\n",
    "\n",
    "\n",
    "14. How does the choice of distance metric affect the performance of KNN?\n",
    "\n",
    "ANS(14):-\n",
    "In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree.\n",
    "\n",
    "\n",
    "15. Can KNN handle imbalanced datasets? If yes, how?\n",
    "\n",
    "ANS(15):-\n",
    "In imbalanced datasets, kNN becomes biased towards the majority instances of the training space. To solve this problem, we propose a method called Proximity weighted Evidential kNN classifier.\n",
    "\n",
    "\n",
    "16. How do you handle categorical features in KNN?\n",
    "\n",
    "ANS(16):-by using methods such as onehot encoding technique ,label encoding and  ordinal encoding.\n",
    "\n",
    "\n",
    "\n",
    "17. What are some techniques for improving the efficiency of KNN?\n",
    "\n",
    "ANS(17):-In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model.\n",
    "\n",
    "\n",
    "\n",
    "18. Give an example scenario where KNN can be applied.\n",
    "\n",
    "ANS(18):-With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Clustering:\n",
    "\n",
    "19. What is clustering in machine learning?\n",
    "\n",
    "ANS(19):-In machine learning too, we often group examples as a first step to understand a subject (data set) in a machine learning system. Grouping unlabeled examples is called clustering. As the examples are unlabeled, clustering relies on unsupervised machine learning.\n",
    "\n",
    "\n",
    "\n",
    "20. Explain the difference between hierarchical clustering and k-means clustering.\n",
    "\n",
    "ANS(20):-k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster.\n",
    "\n",
    "\n",
    "\n",
    "21. How do you determine the optimal number of clusters in k-means clustering?\n",
    "\n",
    "ANS(21):-Average silhouette method\n",
    "Compute clustering algorithm (e.g., k-means clustering) for different values of k. ...\n",
    "For each k, calculate the average silhouette of observations (avg. ...\n",
    "Plot the curve of avg. ...\n",
    "The location of the maximum is considered as the appropriate number of clusters.\n",
    "\n",
    "\n",
    "\n",
    "22. What are some common distance metrics used in clustering?\n",
    "\n",
    "ANS(22):-\n",
    "The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance.\n",
    "\n",
    "\n",
    "23. How do you handle categorical features in clustering?\n",
    "\n",
    "ANS(23):-One-Hot Encoding. One way to handle categorical variables is to use one-hot encoding. ...\n",
    "K-Modes Clustering. K-means clustering is a popular clustering algorithm, but it is not directly applicable to categorical data. ...\n",
    "Mixed Clustering.\n",
    "\n",
    "\n",
    "\n",
    "24. What are the advantages and disadvantages of hierarchical clustering?\n",
    "\n",
    "ANS(24):-The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets. And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "25. Explain the concept of silhouette score and its interpretation in clustering.\n",
    "\n",
    "ANS(25):-The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "\n",
    "\n",
    "26. Give an example scenario where clustering can be applied.\n",
    "\n",
    "ANS(26):-Some examples of clustering include document clustering, fraud detection, fake news detection, customer segmentation, etc. This article lists some exciting and unique clustering projects in machine learning that will help you understand the real-world applications of clustering.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Anomaly Detection:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "27. What is anomaly detection in machine learning?\n",
    "\n",
    "ANS(27):-\n",
    "Anomaly detection is a process of finding those rare items, data points, events, or observations that make suspicions by being different from the rest data points or observations. Anomaly detection is also known as outlier detection.\n",
    "\n",
    "\n",
    "28. Explain the difference between supervised and unsupervised anomaly detection.\n",
    "\n",
    "ANS(28):-The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching.\n",
    "\n",
    "\n",
    "\n",
    "29. What are some common techniques used for anomaly detection?\n",
    "\n",
    "ANS(29):-\n",
    "Some of the popular techniques are:\n",
    "Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
    "Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
    "Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data.\n",
    "\n",
    "\n",
    "30. How does the One-Class SVM algorithm work for anomaly detection?\n",
    "\n",
    "ANS(30):-One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method.\n",
    "\n",
    "\n",
    "\n",
    "31. How do you choose the appropriate threshold for anomaly detection?\n",
    "\n",
    "ANS(31):-For Anomaly detection threshold, choose the number to use for the anomaly detection threshold. A higher number creates a thicker band of \"normal\" values that is more tolerant of metric changes. A lower number creates a thinner band that will go to ALARM state with smaller metric deviations.\n",
    "\n",
    "\n",
    "\n",
    "32. How do you handle imbalanced datasets in anomaly detection?\n",
    "\n",
    "ANS(32):-\n",
    "SMOTE\n",
    "UNDERSAMPLING \n",
    "OVER SAMPLING\n",
    "\n",
    "\n",
    "33. Give an example scenario where anomaly detection can be applied.\n",
    "\n",
    "ANS(33):-For example, a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dimension Reduction:\n",
    "\n",
    "34. What is dimension reduction in machine learning?\n",
    "\n",
    "ANS(34):-Dimensionality reduction is the task of reducing the number of features in a dataset. In machine learning tasks like regression or classification, there are often too many variables to work with. These variables are also called features.\n",
    "\n",
    "\n",
    "\n",
    "35. Explain the difference between feature selection and feature extraction.\n",
    "\n",
    "ANS(35):-The main difference between them is that feature selection is about selecting the subset of the original feature set, whereas feature extraction creates new features. Feature selection is a way of reducing the input variable for the model by using only relevant data in order to reduce overfitting in the model.\n",
    "\n",
    "\n",
    "\n",
    "36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
    "\n",
    "ANS(36):-“PCA works on a condition that while the data in a higher-dimensional space is mapped to data in a lower dimension space, the variance or spread of the data in the lower dimensional space should be maximum.”\n",
    "\n",
    "\n",
    "\n",
    "37. How do you choose the number of components in PCA?\n",
    "\n",
    "ANS(37):-If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset.\n",
    "\n",
    "\n",
    "\n",
    "38. What are some other dimension reduction techniques besides PCA?\n",
    "\n",
    "ANS(38):-Dimensionality Reduction Techniques\n",
    "Feature selection. ...\n",
    "Feature extraction. ...\n",
    "Principal Component Analysis (PCA) ...\n",
    "Non-negative matrix factorization (NMF) ...\n",
    "Linear discriminant analysis (LDA) ...\n",
    "Generalized discriminant analysis (GDA) ...\n",
    "Missing Values Ratio. ...\n",
    "Low Variance Filter.\n",
    "\n",
    "\n",
    "\n",
    "39. Give an example scenario where dimension reduction can be applied.\n",
    "\n",
    "ANS(39):-\n",
    "For example, maybe we can combine Dum Dums and Blow Pops to look at all lollipops together. Dimensionality reduction can help in both of these scenarios. There are two key methods of dimensionality reduction: Feature selection: Here, we select a subset of features from the original feature set.\n",
    "\n",
    "\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "40. What is feature selection in machine learning?\n",
    "\n",
    "ANS(40):-Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve.\n",
    "\n",
    "\n",
    "41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
    "\n",
    "ANS(41):-Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model.\n",
    "\n",
    "\n",
    "42. How does correlation-based feature selection work?\n",
    "\n",
    "ANS(42):-Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features\n",
    "\n",
    "\n",
    "43. How do you handle multicollinearity in feature selection?\n",
    "\n",
    "ANS(43):-To address multicollinearity, techniques such as regularization or feature selection can be applied to select a subset of independent variables that are not highly correlated with each other. In this article, we will focus on the most common one – VIF (Variance Inflation Factors).\n",
    "\n",
    "\n",
    "44. What are some common feature selection metrics?\n",
    "\n",
    "ANS(44):-\n",
    "Types of Feature Selection Methods in ML\n",
    "Chi-square Test. The Chi-square test is used for categorical features in a dataset. ...\n",
    "Fisher's Score. ...\n",
    "Correlation Coefficient. ...\n",
    "Dispersion Ratio. ...\n",
    "Backward Feature Elimination. ...\n",
    "Recursive Feature Elimination. ...\n",
    "Random Forest Importance.\n",
    "\n",
    "45. Give an example scenario where feature selection can be applied.\n",
    "\n",
    "ANS(45):-\n",
    "Below are some real-life examples of feature selection: Mammographic image analysis. Criminal behavior modeling. Genomic data analysis.\n",
    "\n",
    "\n",
    "Data Drift Detection:\n",
    "\n",
    "46. What is data drift in machine learning?\n",
    "\n",
    "sol(46):-Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use.\n",
    "\n",
    "\n",
    "\n",
    "47. Why is data drift detection important?\n",
    "\n",
    "sol(47):-Data drift is one of the top reasons model accuracy degrades over time. For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues.\n",
    "\n",
    "\n",
    "\n",
    "48. Explain the difference between concept drift and feature drift.\n",
    "\n",
    "sol(48):-Data drift refers to the changing distribution of the data to which the model is applied. Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift can lead to a decline in the performance of a machine learning model.\n",
    "\n",
    "\n",
    "\n",
    "49. What are some techniques used for detecting data drift?\n",
    "\n",
    "sol(49):-Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric.\n",
    "\n",
    "\n",
    "\n",
    "50. How can you handle data drift in a machine learning model?\n",
    "\n",
    "sol(50):-Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data Leakage:\n",
    "\n",
    "51. What is data leakage in machine learning?\n",
    "\n",
    "SOL(51):-\"A scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. It causes high performance while training set, but perform poorly in deployment or production.\"\n",
    "\n",
    "\n",
    "\n",
    "52. Why is data leakage a concern?\n",
    "\n",
    "SOL(52):-A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation.\n",
    "\n",
    "\n",
    "\n",
    "53. Explain the difference between target leakage and train-test contamination.\n",
    "\n",
    "SOL(53):-Target leakage: Also known as data leakage. Target leakage occurs when a variable that is not a feature is used to predict the target. This occurs when the model is built, or trained, with information (known as the training dataset) that will not be available in unseen data.\n",
    "\n",
    "\n",
    "\n",
    "54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
    "\n",
    "\n",
    "SOL(54):-Let's discuss these tips in a detailed manner to minimize or fix data leakage problems in machine learning.\n",
    "Extract the appropriate set of features: ...\n",
    "Add an individual validation set: ...\n",
    "Apply data pre-processing separately to both data sets: ...\n",
    "Time-series data: ...\n",
    "Cross-Validation:\n",
    "\n",
    "\n",
    "\n",
    "55. What are some common sources of data leakage?\n",
    "\n",
    "SOL(55):-8 Most Common Causes of Data Breach\n",
    "Weak and Stolen Credentials, a.k.a. Passwords. ...\n",
    "Back Doors, Application Vulnerabilities. ...\n",
    "Malware. ...\n",
    "Social Engineering. ...\n",
    "Too Many Permissions. ...\n",
    "Insider Threats. ...\n",
    "Physical Attacks. ...\n",
    "Improper Configuration, User Error.\n",
    "\n",
    "\n",
    "56. Give an example scenario where data leakage can occur.\n",
    "\n",
    "SOL(56):-\n",
    "Data leakage occurs when sensitive data gets unintentionally exposed to the public in transit, at rest, or in use. Here are common examples: Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications.\n",
    "\n",
    "\n",
    "Cross Validation:\n",
    "\n",
    "57. What is cross-validation in machine learning?\n",
    "\n",
    "SOL(54):-Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern.\n",
    "\n",
    "\n",
    "58. Why is cross-validation important?\n",
    "\n",
    "SOL(54):-The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model's generalizability. It also helps to avoid overfitting, which is a common problem in machine learning.\n",
    "\n",
    "\n",
    "59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
    "\n",
    "SOL(54):-KFold is a cross-validator that divides the dataset into k folds. Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
    "\n",
    "\n",
    "60. How do you interpret the cross-validation results?\n",
    "\n",
    "SOL(54):-Leave One Out Cross Validation (LOOCV):\n",
    "This is repeated for all combinations in which the original sample can be separated this way, and then the error is averaged for all trials, to give overall effectiveness. The number of possible combinations is equal to the number of data points in the original sample or n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
